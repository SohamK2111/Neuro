{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3df08f-d36d-4ae8-94b7-9062b1e5eca0",
   "metadata": {},
   "source": [
    "# IIA project GG3: Neural Data Analysis\n",
    "\n",
    "Easter 2023<br>\n",
    "Project Leader: Yashar Ahmadian (ya311)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a402b4d-bab9-48fa-b0a3-1560629e142a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Forward-Backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324af30a-8ce9-49ae-903a-9c682fd45f3b",
   "metadata": {},
   "source": [
    "$\\newcommand{\\valpha}{\\vec{\\alpha}}$\n",
    "$\\newcommand{\\vbeta}{\\vec{\\beta}}$\n",
    "$\\newcommand{\\talpha}{\\tilde{\\alpha}}$\n",
    "$\\newcommand{\\tbeta}{\\tilde{\\beta}}$\n",
    "$\\newcommand{\\T}{\\mathcal{T}}$\n",
    "$\\newcommand{\\J}{\\mathcal{J}}$\n",
    "\n",
    "The forward backward algorithm is a message passing algorithm (and an example of dynamic programming) used\n",
    "for calculating the posterior probabilities of the hidden states of a HMM at different times, conditioned on a sequence of observations. For us we denote the hidden states by $s_t$ and the observations by $n_t$. \n",
    "\n",
    "The goal is to calculate the posterior probability $P(s_t | n_{1:T})$. By the definition of conditional probability,\n",
    "this is given by $P(s_t , n_{1:T})/ P(n_{1:T})$. Thus up to normalization (found by summing the probability over the $K$ values of $s_t$) we need to evaluate the joint probability $P(s_t, n_{1:T})$. By the product rule of probability theory, the latter can be written as the product of $P(n_{t+1:T} | s_t)$ and $P(s_t, n_{1:t})$ (in addition to the product rule,Â we have also used the Markov property to replace $P(n_{t+1:T}| s_t, n_{1:t})$ with $P(n_{t+1:T} | s_t)$.\n",
    "If we define\n",
    "\n",
    "$\\alpha_t^s := P(s_t =s, n_{1:t})$\n",
    "\n",
    "$\\beta_t^s := P(n_{t+1:T} | s_t=s)$\n",
    "\n",
    "We thus have \n",
    "\n",
    "$P(s_t = s| n_{1:T}) \\propto \\alpha_t^s\\,\\, \\beta_t^s$.\n",
    "\n",
    "The gain here is that $\\valpha_t$ and $\\vbeta_t$ (both $K$ dimensional vectors with components $\\alpha_t^s$ and $\\beta_t^s$) satisfy recursion relations that can be used to compute them. These recursions can be derived by starting from the definitions of $\\valpha_t$ and $\\vbeta_t$, given above, and using the sum and product rules of probability theory, as well as the Markov property of the model. If we define $l_{t}^s$ to denote the conditional observation probability $P(n_t| s_t = s)$, we then find (see below for proof):\n",
    "\n",
    "$\\alpha_{t+1}^s = l_{t+1}^s \n",
    "\\sum_{s'=1}^K \\T_{s,s'} \\alpha_{t}^{s'}\n",
    "\\qquad \\qquad\\quad$          (1)\n",
    "\n",
    "and \n",
    "\n",
    "$\\beta_{t}^s =\n",
    "\\sum_{s'=1}^K \\beta_{t+1}^{s'} l_{t+1}^{s'} \\T_{s',s} \n",
    "\\qquad \\qquad\\qquad$          (2)\n",
    "\n",
    "where $s'$ is summed over the $K$ possible states, in each case. Note that the $\\alpha$-recursion goes forward in time, while the $\\beta$-recursion goes backwards; hence the name of the algorith. \n",
    "\n",
    "Defining the time-dependent matrix $\\J_t$ via $[\\J_t]_{s,s'} = l_{t}^s \\T_{s,s'}$, and assuming $\\valpha$'s and $\\vbeta$'s and column and row vectors, respectively, we can write the recursion relations more compactly as \n",
    "\n",
    "$\n",
    "\\valpha_{t+1} = \\J_{t+1} \\valpha_t\n",
    "$\n",
    "\n",
    "$\n",
    "\\vbeta_{t} = \\vbeta_{t+1}\\J_{t+1}\n",
    "$\n",
    "\n",
    "which also shows these recursions form discrete-time linear time-inhomogeneous systems. \n",
    "\n",
    "Finally, the initial conditions for the two recursions are\n",
    "\n",
    "$\\alpha_{t=1}^s = l_{t=1}^s \\pi^s$\n",
    "\n",
    "$\\beta_{t=T}^s = 1$\n",
    "\n",
    "The first follows immediately from the definition, $\\alpha_1^s = P(n_1, s_1=s) = P(n_1 | s_1=s) P(s_1=s)$. The second follows because the set of future observation at the last time step, $n_{T+1:\\ldots}$, is empty and thus its (conditional) probability (given $s_T$) is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a78a1-20be-4e94-97ae-20508c696e48",
   "metadata": {},
   "source": [
    "\n",
    "**Proof of the recursion equations:** To prove (1), we start from the definition of $\\valpha_t$, and use the sum rule  followed by the product rule to write\n",
    "\n",
    "$\n",
    "\\alpha_{t+1}^s = P(s_{t+1} = s, n_{t+1}, n_{1:t}) = \\sum_{s'} P(s_{t+1}=s, n_{t+1}, s_t=s', n_{1:t})\n",
    "$\n",
    "\n",
    "$\n",
    "\\qquad = \\sum_{s'} P(s_{t+1}=s, n_{t+1} | s_t=s', n_{1:t})\\, P( s_t=s', n_{1:t}) \n",
    "$\n",
    "\n",
    "$ \\qquad = \\sum_{s'} P(s_{t+1}=s, n_{t+1} | s_t=s', n_{1:t})\\,\\, \\alpha_{t+1}^{s'}\n",
    "\\qquad\\qquad$ (3)\n",
    "\n",
    "Now due to the *Markov property*, conditioning on   $n_{1:t}$ can be dropped in the left factor of the summands in the last expression:\n",
    "$P(s_{t+1}=s, n_{t+1} | s_t=s', n_{1:t}) = P(s_{t+1}=s, n_{t+1} | s_t=s')$. Using the product rule one more time, we  write this as \n",
    "$P(n_{t+1}| s_{t+1}=s, s_t=s') P(s_{t+1}=s| s_t=s')$. We then use the *conditional independence property of the observations* (CIPO) in the HMM (i.e. the fact that conditioned on $s_{t+1}$, $n_{t+1}$ is independent of $s_t$), to drop the conditioning on $s_t$ in the left factor, substitute in (3), and obtain \n",
    "\n",
    "$ \\alpha_{t+1}^s = = \\sum_{s'} P(n_{t+1}| s_{t+1}=s) P(s_{t+1}=s| s_t=s')\\,\\, \\alpha_{t+1}^{s'}\n",
    "$\n",
    "\n",
    "Finally, plugging in the definitions $l_{t}^s = P(n_t | s_t = s)$ and $\\T_{ss'} = P(s_{t+1}=s| s_t=s')$, we obtain (1). \n",
    "\n",
    "\n",
    "To prove (2), start with the definition of $\\vbeta_t$, then use the sum rule, the product rule, and then the *Markov property* to write:\n",
    "$\\beta_{t}^s = P(n_{t+1:T}| s_t=s) = \\sum_{s'} P(n_{t+1:T}, s_{t+1}=s' | s_t=s)$\n",
    "\n",
    "$\\qquad = \\sum_{s'} P(n_{t+1:T}| s_{t+1}=s' , s_t=s)P(s_{t+1}=s' | s_t=s)$\n",
    "\n",
    "$\\qquad = \\sum_{s'} P(n_{t+1:T}| s_{t+1}=s')P(s_{t+1}=s' | s_t=s)$\n",
    "\n",
    "Using the product rule and the Markov property again, we can write the left factors as $P(n_{t+1:T}| s_{t+1}=s') = P(n_{t+2:T}| s_{t+1}=s', n_{t+1})P(n_{t+1}| s_{t+1}=s') = P(n_{t+2:T}| s_{t+1}=s')P(n_{t+1}| s_{t+1}=s') $. Substituting this expression, together with the definitions of $\\vbeta_{t+1}$, $\\vec{l}_{t}$ and $\\T$ we obtain (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedeffd-3496-4c86-9e8b-2837963ad655",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Code implementation:\n",
    "\n",
    "The code in `inference.py` (adapted from the [SSM package](https://github.com/lindermanlab/ssm) by the Linderman lab) implements the above recursions in terms of the logs of the $\\valpha$, $\\vbeta$, and the observation and transition probabilities. If we let $\\talpha$,  $\\tbeta$, $\\tilde \\T$ and $ll_t$ denote the logs of $\\alpha$, $\\beta$, $\\T$ and $l_t$, can write the recursion equations as \n",
    "\n",
    "$\\talpha_{t+1}^s = ll_{t+1}^s  + \\log \n",
    "\\sum_{s'=1}^K \\exp( \\tilde{T}_{s,s'} + \\talpha_{t+1}^{s'})\n",
    "$\n",
    "\n",
    "and \n",
    "\n",
    "$\\tbeta_{t}^s = \\log\n",
    "\\sum_{s'=1}^K \\exp(\\tbeta_{t+1}^{s'} + ll_{t+1}^{s'} + \\tilde{T}_{s',s})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6932d6-0a65-46ef-8d99-a50b2b01771c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model log-likelihood\n",
    "\n",
    "By the definition of $\\alpha_T^s = P(s_T=s, n_{1:T})$, and if we sum this over all $s$, we obtain $P(n_{1:T})$. \n",
    "Recalling that model parameters, $\\Theta$, where implicitly conditioned on in all of the above probabilities, the latter probability is nothing but the model likelihood \n",
    "\n",
    "$P(\\text{observed data} | \\Theta) = P(n_{1:T} | \\Theta)$.\n",
    "\n",
    "Thus the model likelihood can also be calculated using only the forward pass half of the forward-backward algorithm.\n",
    "\n",
    "The function `hmm_normalizer` of `inference.py` calculates the model **log-** likelihood using the forward pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
